{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybylmep3wivS"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssf4Y1uMv60A"
      },
      "source": [
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medsiglip/blob/main/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedsiglip%2Fmain%2Fnotebooks%2Ftrain_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medsiglip/blob/main/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/google/medsiglip-448\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "# Classifying skin conditions with MedSigLIP and SCIN dataset\n",
        "\n",
        "In this notebook we will train a model to classify skin conditions from images in the [SCIN dataset](https://github.com/google-research-datasets/scin) based on embeddings generated by the [MedSigLIP model](https://developers.google.com/health-ai-developer-foundations/medsiglip).\n",
        "\n",
        "\n",
        "The SCIN (Skin Condition Image Network) open access dataset contains 5,000+ volunteer contributions (10,000+ images) of common dermatology conditions. The SCIN dataset was collected from Google Search users in the United States through a voluntary, consented image donation application. Three dermatologists labeled each image with up to 3 conditions, and a confidence rating from 1-5 for each condition. For example:\n",
        "\n",
        "Dermatologist labels:\n",
        "- Dermatologist 1's label: Eczema with confidence 3, Acute and chronic dermatitis with confidence 2, Psoriasis vulgaris with confidence 1\n",
        "\n",
        "- Dermatologist 2's label: Eczematous dermatitis with confidence 3\n",
        "\n",
        "- Dermatologist 3's label: Eczematous dermatitis with confidence 4, Post-inflammatory hyperpigmentation with confidence 3\n",
        "\n",
        "The labels would be as follows:\n",
        "\n",
        "- dermatologist_skin_condition_label_name: Eczema, Acute and chronic dermatitis, Psoriasis vulgaris, Eczematous dermatitis, Eczematous dermatitis, Post-inflammatory hyperpigmentation\n",
        "\n",
        "- dermatologist_skin_condition_confidence: [3, 2, 1, 3, 4, 3]  \n",
        "\n",
        "The MedSigLIP model is used to generate rich embeddings for medical images allowing us to train a machine learning model with less data and compute compared to training from scratch. Visit the [MedSigLIP page](https://developers.google.com/health-ai-developer-foundations/medsiglip) on the HAI-DEF site to learn more about the model and see this notebook to learn more about the [SCIN dataset](https://github.com/google-research-datasets/scin/blob/main/scin_demo.ipynb).\n",
        "\n",
        "We will frame this as a multi-label classification problem, where the model takes in a 6144 dimensional embedding and the label is which of the following skin conditions the patient had `['Eczema', 'Allergic Contact Dermatitis', 'Insect Bite', 'Urticaria', 'Psoriasis', 'Folliculitis', 'Irritant Contact Dermatitis', 'Tinea', 'Herpes Zoster', 'Drug Rash']`. Using our example above our label will be `[ 0 1 0 1 0 0 0 0 0 0 ]`, since at least one doctor labeled the conditions Eczema and Psoriasis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZXH0rumwbG"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This notebook uses precomputed embeddings by default and can be run using a CPU runtime. If you are generating the embeddings, you can use a runtime with a GPU to speed up generation:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHbC0-JAxlR2"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import io\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage\n",
        "\n",
        "# @markdown Set `USE_PRECOMPUTED_EMBEDDINGS` to load precomputed embeddings (True by default). Uncheck this option if you want to generate the embeddings.\n",
        "USE_PRECOMPUTED_EMBEDDINGS = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqASBgOzDHD"
      },
      "source": [
        "## Load SCIN dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whVr9XaUzBz5"
      },
      "outputs": [],
      "source": [
        "SCIN_GCS_BUCKET_NAME = 'dx-scin-public-data'\n",
        "SCIN_GCS_CASES_CSV = 'dataset/scin_cases.csv'\n",
        "SCIN_GCS_LABELS_CSV = 'dataset/scin_labels.csv'\n",
        "SCIN_GCS_IMAGES_DIR = 'dataset/images/'\n",
        "\n",
        "\n",
        "def initialize_df_with_metadata(bucket, csv_path):\n",
        "  \"\"\"Loads the given CSV into a pd.DataFrame.\"\"\"\n",
        "  df = pd.read_csv(io.BytesIO(bucket.blob(csv_path).download_as_string()), dtype={'case_id': str})\n",
        "  df['case_id'] = df['case_id'].astype(str)\n",
        "  return df\n",
        "\n",
        "\n",
        "def augment_metadata_with_labels(df, bucket, csv_path):\n",
        "  \"\"\"Loads the given CSV into a pd.DataFrame.\"\"\"\n",
        "  labels_df = pd.read_csv(io.BytesIO(bucket.blob(csv_path).download_as_string()), dtype={'case_id': str})\n",
        "  print(f'Loaded labels with {len(labels_df)} rows.')\n",
        "  labels_df['case_id'] = labels_df['case_id'].astype(str)\n",
        "  merged_df = pd.merge(df, labels_df, on='case_id')\n",
        "  return merged_df\n",
        "\n",
        "\n",
        "scin_bucket = storage.Client.create_anonymous_client().bucket(SCIN_GCS_BUCKET_NAME)\n",
        "\n",
        "scin_no_label_df = initialize_df_with_metadata(scin_bucket, SCIN_GCS_CASES_CSV)\n",
        "scin_df = augment_metadata_with_labels(scin_no_label_df, scin_bucket, SCIN_GCS_LABELS_CSV)\n",
        "scin_df.set_index('case_id', inplace=True)\n",
        "print(f'Loaded {len(scin_df)} rows.')\n",
        "\n",
        "# scin_df is the main data frame we will be working with.\n",
        "scin_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlPyw_xlBWsT"
      },
      "source": [
        "## Explore SCIN dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6jbcY05BXog"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "\n",
        "def display_image(bucket, image_path):\n",
        "  image = Image.open(io.BytesIO(bucket.blob(image_path).download_as_string()))\n",
        "  f, axarr = plt.subplots(1, 1, figsize = (4, 4))\n",
        "  axarr.imshow(image, cmap='gray')\n",
        "  axarr.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_images_for_case(df, case_id):\n",
        "  # Each volunteer contributor submitted up to 3 images\n",
        "  image_paths = [df.loc[case_id, 'image_1_path'], df.loc[case_id, 'image_2_path'], df.loc[case_id, 'image_3_path']]\n",
        "  for path in image_paths:\n",
        "    if isinstance(path, str):\n",
        "      scin_bucket = storage.Client.create_anonymous_client().bucket(SCIN_GCS_BUCKET_NAME)\n",
        "      display_image(scin_bucket, path)\n",
        "\n",
        "\n",
        "  conditions = df.loc[case_id, 'dermatologist_skin_condition_on_label_name']\n",
        "  confidence = df.loc[case_id, 'dermatologist_skin_condition_confidence']\n",
        "  print(f'Skin Conditions {conditions}')\n",
        "  print(f'Confidence {confidence}')\n",
        "\n",
        "\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        colab_output.clear()\n",
        "        display(case_id_dropdown)\n",
        "        display_images_for_case(scin_df,case_id=change['new'])\n",
        "\n",
        "\n",
        "case_id_dropdown = widgets.Dropdown(options=scin_df.index, description=\"Case ID\")\n",
        "display(case_id_dropdown)\n",
        "case_id_dropdown.observe(on_change)\n",
        "display_images_for_case(scin_df, case_id_dropdown.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YPakpYYBeLY"
      },
      "outputs": [],
      "source": [
        "def print_condition_distribution(df, top_n_conditions=50):\n",
        "  # Any condition that shows up in a label\n",
        "  condition_ctr = collections.Counter()\n",
        "  print(f'Distribution of conditions in \"dermatologist_skin_condition_on_label_name\" column:')\n",
        "  for entry in df['dermatologist_skin_condition_on_label_name'].dropna():\n",
        "    condition_ctr.update(eval(entry))\n",
        "  for condition, cnt in condition_ctr.most_common()[:top_n_conditions]:\n",
        "    print(f'  {condition}: {cnt}')\n",
        "\n",
        "\n",
        "print_condition_distribution(scin_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhhHrkkBgwL"
      },
      "source": [
        "## Clean and prepare the data\n",
        "\n",
        "We will try and predict the 10 most common conditions:\n",
        "\n",
        "`['Eczema', 'Allergic Contact Dermatitis', 'Insect Bite', 'Urticaria', 'Psoriasis', 'Folliculitis', 'Irritant Contact Dermatitis', 'Tinea', 'Herpes Zoster', 'Drug Rash']`\n",
        "\n",
        "\n",
        "Our training data X will be a list of embeddings of size`(6144,)` and our labels y will be a list of binary labels of size `(10,)`. For example, `[0, 1, 0, 0, 1, 0, 1, 0, 0, 1]`.\n",
        "\n",
        "We filter examples if the dermatologists labeled it with insufficient image quality. We also filter labels that are below our minimum confidence. For example, if we set minimum confidence to 3 and the dermatologist labeled Eczema with confidence 2, we included the example, but Eczma will be set to 0 not 1 in our label.\n",
        "\n",
        "Finally, the skin condition labels are imbalanced. For example we have 156 examples with drug rash and 6295 examples without. Typically, you want to split your data such that there is an even distribution of positive labels for each condition in the train and test sets. We will print out the distributions, but don't explicitly create even distributions in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBUvxHwcBkBP"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "CONDITIONS_TO_PREDICT = ['Eczema', 'Allergic Contact Dermatitis', 'Insect Bite', 'Urticaria', 'Psoriasis', 'Folliculitis', 'Irritant Contact Dermatitis', 'Tinea', 'Herpes Zoster', 'Drug Rash']\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "  MINIMUM_CONFIDENCE = 0\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  poor_image_quality_counter = 0\n",
        "  missing_embedding_counter = 0\n",
        "  not_in_condition_to_predict_counter = 0\n",
        "  condition_confidence_low_counter = 0\n",
        "\n",
        "  for row in scin_df.itertuples():\n",
        "    if row.dermatologist_gradable_for_skin_condition_1 != 'DEFAULT_YES_IMAGE_QUALITY_SUFFICIENT':\n",
        "      poor_image_quality_counter += 1\n",
        "      continue\n",
        "\n",
        "    # eval converts from string to dict\n",
        "    labels = eval(row.dermatologist_skin_condition_on_label_name)\n",
        "    confidence = eval(row.dermatologist_skin_condition_confidence)\n",
        "\n",
        "    row_labels = []\n",
        "    for label, confidence in zip(labels, confidence):\n",
        "      if label not in CONDITIONS_TO_PREDICT:\n",
        "        not_in_condition_to_predict_counter += 1\n",
        "        continue\n",
        "      if confidence < MINIMUM_CONFIDENCE:\n",
        "        condition_confidence_low_counter += 1\n",
        "        continue\n",
        "      row_labels.append(label)\n",
        "\n",
        "    for image_path in [row.image_1_path, row.image_2_path, row.image_3_path]:\n",
        "      if pd.isna(image_path):\n",
        "        continue\n",
        "\n",
        "      X.append(image_path)\n",
        "      y.append(row_labels)\n",
        "\n",
        "\n",
        "  print(f'Poor image quality: {poor_image_quality_counter}')\n",
        "  print(f'Missing embedding: {missing_embedding_counter}')\n",
        "  print(f'Condition not in \"CONDITIONS_TO_PREDICT\": {not_in_condition_to_predict_counter}')\n",
        "  print(f'Exluded label confidence too low: {condition_confidence_low_counter}')\n",
        "  return X, y\n",
        "\n",
        "X_image_paths, y = prepare_data()\n",
        "# Convert y from [['Eczma'], ['Urticaria', 'Insect Bite']] to\n",
        "# [[0 0 1 0 0 0 0 0 0 0], [0 0 0 1 1 0 0 0 0 0]]\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes = CONDITIONS_TO_PREDICT)\n",
        "y = mlb.fit_transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQFXiZRSKnQi"
      },
      "source": [
        "## Load precomputed embeddings\n",
        "\n",
        "Since it takes ~hours to generate the embeddings, we will download precomputed embeddings from Google Cloud. If you would like to generate your own, see [Compute embeddings in batch](#scrollTo=iMu1zWYMPJWK)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omA0LbmXoVJ0"
      },
      "outputs": [],
      "source": [
        "if USE_PRECOMPUTED_EMBEDDINGS:\n",
        "  from google.colab import auth\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import tensorflow as tf  # Or any other library that uses GCS\n",
        "\n",
        "  file_path = 'gs://healthai-us/medsiglip/scin_medsiglip_embeddings_and_binarized_labels.npz'\n",
        "\n",
        "  try:\n",
        "      data = np.load(tf.io.gfile.GFile(file_path, 'rb'), allow_pickle=True)\n",
        "      print(type(data))\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading the file: {e}\")\n",
        "\n",
        "  d = data['data']\n",
        "  X = np.vstack(d[:, 1])\n",
        "  Y = np.vstack(d[:, 2])\n",
        "\n",
        "  print(f'Length of X: {len(X)}')\n",
        "  print(f'Length of Y: {len(Y)}')\n",
        "  print(f'Sample from X: {X[0].shape}')\n",
        "  print(f'Sample from y: {Y[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMu1zWYMPJWK"
      },
      "source": [
        "## (Optional) Compute embeddings in batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOEkuHOtN_RK"
      },
      "outputs": [],
      "source": [
        "if not USE_PRECOMPUTED_EMBEDDINGS:\n",
        "  # Authenticate user for HuggingFace if needed. Enter token below if requested.\n",
        "  from huggingface_hub.utils import HfFolder\n",
        "  from huggingface_hub import notebook_login\n",
        "\n",
        "  if HfFolder.get_token() is None:\n",
        "      notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zWHUzhgGLa9"
      },
      "outputs": [],
      "source": [
        "if not USE_PRECOMPUTED_EMBEDDINGS:\n",
        "  import requests\n",
        "  from transformers import AutoProcessor, AutoModel  # Run this import only once\n",
        "  import torch\n",
        "\n",
        "  batch_size = 128  # Adjust this based on memory. Start small and increase if possible.\n",
        "  num_images = len(X_image_paths)\n",
        "\n",
        "  model_id = \"google/medsiglip-448\"  # @param {type:\"string\"}\n",
        "\n",
        "  model = AutoModel.from_pretrained(model_id, device_map=\"auto\")\n",
        "  processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "  embeddings = [] # List to hold the embeddings\n",
        "  labels = [] # list to hold the labels\n",
        "\n",
        "  for i in range(0, num_images, batch_size):\n",
        "      print(f'Processing {i} to {i + batch_size}')\n",
        "      batch_paths = X_image_paths[i:i + batch_size]\n",
        "      batch_labels = y[i:i + batch_size]\n",
        "\n",
        "      X_images =  [] # List to hold the images for the current batch\n",
        "      y_labels = []\n",
        "\n",
        "      for image_path, row_labels in zip(batch_paths, batch_labels):\n",
        "          try:\n",
        "            response = requests.get(\"https://storage.googleapis.com/dx-scin-public-data/\" + image_path, stream=True)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
        "            image = Image.open(io.BytesIO(response.content))\n",
        "            X_images.append(image)\n",
        "            y_labels.append(row_labels)\n",
        "          except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching image from {image_path}: {e}\")\n",
        "          except Exception as e: # Catch other potential exceptions during image processing\n",
        "            print(f\"Error processing image from {image_path}: {e}\")\n",
        "      if not X_images: # Skip empty batches.\n",
        "          continue\n",
        "\n",
        "      print(len(X_images), len(y_labels))\n",
        "      inputs = processor(images=X_images, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          image_features = model.get_image_features(**inputs)\n",
        "\n",
        "      embeddings.append(image_features.cpu().numpy()) # Convert to NumPy\n",
        "      labels.append(y_labels) # Add labels to the Y list\n",
        "\n",
        "  final_embeddings = np.concatenate(embeddings, axis=0)\n",
        "  labels = np.concatenate(labels, axis=0)\n",
        "  print(final_embeddings.shape)  # Print the shape of the final embeddings\n",
        "\n",
        "  # Now you have 'final_embeddings', a NumPy array containing the embeddings for all images.\n",
        "  # You can save it to a file:\n",
        "  # np.save(\"image_embeddings.npy\", final_embeddings)\n",
        "\n",
        "  X = final_embeddings\n",
        "  Y = labels\n",
        "  print(f'Length of X: {len(X)}')\n",
        "  print(f'Length of Y: {len(Y)}')\n",
        "  print(f'Sample from X: {X[0].shape}')\n",
        "  print(f'Sample from y: {Y[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy4p8Xtec1CZ"
      },
      "source": [
        "## Create train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFKZlu_YCeG9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def print_label_distribution(y, mlb):\n",
        "    \"\"\"Prints the distribution of labels.\"\"\"\n",
        "\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Count label occurrences\n",
        "    label_counts = np.sum(y, axis=0)\n",
        "\n",
        "    # Calculate and print the distribution\n",
        "    label_percentages = label_counts / len(y)\n",
        "\n",
        "    for i, condition in enumerate(mlb.classes_):\n",
        "        print(f\"{condition}: {label_percentages[i]:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nPercentage of positive labels by condition in train:\")\n",
        "print_label_distribution(y_train, mlb)\n",
        "print(\"\\nPercentage of positive labels by condition in test:\")\n",
        "print_label_distribution(y_test, mlb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO9SlTXuCf-U"
      },
      "source": [
        "## Train a logistic regression classifier\n",
        "\n",
        "Using the MultiOutputClassifier wrapper sklearn will train 10 different logistic regression models. One for each of our labels. Let's see how they do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNne8tS1CinP"
      },
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_classifier = MultiOutputClassifier(LogisticRegression(max_iter=250)).fit(X_train, y_train)\n",
        "y_pred = lr_classifier.predict_proba(X_test)\n",
        "\n",
        "# The predict_proba are returned in a funky format so reconfigure to (1291,10)\n",
        "cols = []\n",
        "for i in range(len(mlb.classes_)):\n",
        "  cols.append(y_pred[i][:,1])\n",
        "y_pred = np.column_stack(cols)\n",
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMvKvkVQCn_v"
      },
      "source": [
        "## Evaluate results\n",
        "\n",
        "We will use the [Hamming loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html) instead of accuracy to get a general sense of how our model is performing. For accuracy the predictions must exactly match the labels, so if even one of the 10 predictions are wrong the whole example is marked as incorrect. The hamming loss is the fraction of labels that are incorrectly predicted, which is more forgiving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbr70-GfCnu2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, hamming_loss, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_test, y_pred, classes):\n",
        "  y_bool = (y_pred >= 0.5).astype(int)\n",
        "  cnf_matrix = multilabel_confusion_matrix(y_test, y_bool)\n",
        "  _, axes = plt.subplots(2, 5, figsize=(14, 6), tight_layout=True)\n",
        "  for cf, cl, ax in zip(cnf_matrix, classes, axes.flatten()):\n",
        "    ax.set_title(cl)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cf)\n",
        "    disp.plot(ax=ax)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def print_hamming_loss(y_test, y_pred):\n",
        "  y_bool = (y_pred >= 0.5).astype(int)\n",
        "  print(f'\\n### Hamming Loss: {hamming_loss(y_test, y_bool)} ###')\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_test, y_pred, classes):\n",
        "  _, axes = plt.subplots(2, 5, figsize=(14, 6), tight_layout=True)\n",
        "  for i, (cl, ax) in enumerate(zip(classes, axes.flatten())):\n",
        "    ax.set_title(cl)\n",
        "    RocCurveDisplay.from_predictions(y_test[:, i], y_pred[:, i], ax=ax)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msaa3SzUCxQU"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_test, y_pred, mlb.classes_)\n",
        "plot_roc_curve(y_test, y_pred, mlb.classes_)\n",
        "print_hamming_loss(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEr-COH9C7UG"
      },
      "source": [
        "## Train a neural net\n",
        "\n",
        "Let's see if a simple neural network can do better! Since this is a multi-label classification problem we will use a sigmoid activation function instead of softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qca6KOvNC_PS"
      },
      "outputs": [],
      "source": [
        "# Convert to tensorflow datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "for x, y in train_ds.take(1):\n",
        "    print(\"Input:\", x)\n",
        "    print(\"Target:\", y)\n",
        "\n",
        "train_ds = train_ds.batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCiv0pdtDBd-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "weight_decay = 1e-5\n",
        "\n",
        "inputs = tf.keras.Input(shape=(1152,)) # embedding shape of 1152\n",
        "\n",
        "hidden = layers.Dense(512,\n",
        "                      kernel_regularizer=regularizers.l2(l2=weight_decay),\n",
        "                      bias_regularizer=regularizers.l2(l2=weight_decay),\n",
        "                      activation=\"relu\"\n",
        "                      )(inputs)\n",
        "hidden = layers.Dropout(0.05)(hidden)\n",
        "hidden = layers.Dense(256,\n",
        "                      kernel_regularizer=regularizers.l2(l2=weight_decay),\n",
        "                      bias_regularizer=regularizers.l2(l2=weight_decay),\n",
        "                      activation=\"relu\")(hidden)\n",
        "hidden = layers.Dropout(0.1)(hidden)\n",
        "output = layers.Dense(len(mlb.classes_), activation=\"sigmoid\")(hidden)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs, output)\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "print(model.summary())\n",
        "history = model.fit(\n",
        "    train_ds, validation_data=test_ds, epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFnGhU2DE-w"
      },
      "outputs": [],
      "source": [
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obC2Y9FsDK4Z"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_ds)\n",
        "plot_confusion_matrix(y_test, y_pred, mlb.classes_)\n",
        "plot_roc_curve(y_test, y_pred, mlb.classes_)\n",
        "print_hamming_loss(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx50JxFZWU7g"
      },
      "source": [
        "The neural net did perform better than the logistic regression classifier. We can see this reflected in the reduced Hamming loss which gives us sense of the models performance across all conditions. We can also see improvements in individual AUC's. Eczema, Drug Rash and Psoriasis show improved AUC in this particular run. You might get slightly different numbers because of the non determinism in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCfDcXwP5tJ"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medsiglip/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}